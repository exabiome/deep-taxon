\documentclass{article}

\usepackage{graphicx}
\usepackage[utf8]{inputenc}

\title{Exabiome Journal}
\author{ajtritt }
\date{April 2020}

\begin{document}

\maketitle

\section{Introduction}

\subsubsection*{Wed Apr 22, 2020}

I trained the RozNet network (i.e. AlexNet, adjusted for one-hot-encoded DNA sequences) to predict taxonomy
from 11-dimensional UMAP embeddings (i.e. regression problem) and just class labels (i.e. classification problem).

I tried different dropout rates, removing batch norm at the last layer, using max pooling instead of average pooling
between convolution and fully-connected layers, adding more FC layers, and changing convolution stride. None of these
things appear to impact network performance. I suspect that the model is too complex, and is overfitting the data. I
think this because the test performance never improves and is significantly worse than the train performance. See
\ref{fig:roznet_train_test}.

\subsubsection*{Thu Apr 23, 2020}
Earlier, I also ran a simple neural network (1 Conv layer, 3 fc layers). Train loss bottomed out around 53 MSE, similiar
to what previous networks were achieving. This was after about 35 epochs, before it crashed because sequences a bad
batch could not be passed through the network. This led to me looking into why this problem kep happing, and I discovered
that my sequence storage mechanism of packing bits was not being read correctly. I think this might be why I networks are
not training.


\begin{figure}
  \includegraphics[width=\linewidth]{roznet_results.png}
  \caption{Train and test loss results for RozNet}
  \label{fig:roznet_train_test}
\end{figure}

\end{document}

